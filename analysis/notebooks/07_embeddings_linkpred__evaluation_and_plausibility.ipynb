{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc62922b",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. [Setup and Imports](#setup)\n",
    "2. [Discover Embeddings/LinkPred Artifacts](#discover)\n",
    "3. [Load and Inspect Metrics](#load-metrics)\n",
    "4. [Embedding Quality Analysis](#embedding-quality)\n",
    "5. [Top Predictions Analysis](#top-predictions)\n",
    "6. [Prediction Plausibility](#plausibility)\n",
    "7. [Interpretation](#interpretation)\n",
    "8. [Write Report Outputs](#write-outputs)\n",
    "9. [Reproducibility Notes](#reproducibility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67ff0176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis dir exists: True\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SETUP AND IMPORTS\n",
    "# ============================================================================\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Project paths\n",
    "REPO_ROOT = Path.cwd().parent.parent\n",
    "RESULTS_DIR = REPO_ROOT / \"results\"\n",
    "ANALYSIS_DIR = RESULTS_DIR / \"analysis\"\n",
    "TABLES_DIR = RESULTS_DIR / \"tables\"\n",
    "TABLES_REPORT_DIR = RESULTS_DIR / \"tables\" / \"report\"\n",
    "FIGURES_REPORT_DIR = RESULTS_DIR / \"figures\" / \"report\"\n",
    "WARNINGS_LOG = TABLES_REPORT_DIR / \"_warnings.log\"\n",
    "\n",
    "# Notebook identity\n",
    "NOTEBOOK_ID = \"nb07\"\n",
    "NOTEBOOK_NAME = \"embeddings_linkpred__evaluation_and_plausibility\"\n",
    "\n",
    "# Plotting settings\n",
    "plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Ensure output directories exist\n",
    "TABLES_REPORT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "FIGURES_REPORT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Analysis dir exists: {ANALYSIS_DIR.exists()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c799fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# HELPER FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def append_warning(message: str, notebook_id: str = NOTEBOOK_ID):\n",
    "    \"\"\"Append a warning to the consolidated warnings log.\"\"\"\n",
    "    timestamp = datetime.now().isoformat()\n",
    "    with open(WARNINGS_LOG, \"a\") as f:\n",
    "        f.write(f\"[{timestamp}] [{notebook_id}] {message}\\n\")\n",
    "    print(f\"WARNING: {message}\")\n",
    "\n",
    "def safe_load_parquet(path: Path) -> pl.DataFrame | None:\n",
    "    \"\"\"Safely load a parquet file, returning None if it fails.\"\"\"\n",
    "    try:\n",
    "        return pl.read_parquet(path)\n",
    "    except Exception as e:\n",
    "        append_warning(f\"Failed to load {path.name}: {e}\")\n",
    "        return None\n",
    "\n",
    "def flatten_metrics(metrics_dict: dict, prefix: str = \"\") -> dict:\n",
    "    \"\"\"Flatten nested metrics dictionary.\"\"\"\n",
    "    flat = {}\n",
    "    for k, v in metrics_dict.items():\n",
    "        key = f\"{prefix}{k}\" if prefix else k\n",
    "        if isinstance(v, dict):\n",
    "            flat.update(flatten_metrics(v, f\"{key}__\"))\n",
    "        else:\n",
    "            flat[key] = v\n",
    "    return flat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce703be",
   "metadata": {},
   "source": [
    "<a id=\"discover\"></a>\n",
    "## 2. Discover Embeddings/LinkPred Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4decd6ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 embedding/linkpred artifacts in analysis/:\n",
      "  - airport_embeddings.parquet\n",
      "  - linkpred_metrics.json\n",
      "\n",
      "Found 2 prediction tables in tables/:\n",
      "  - linkpred_top_predictions.csv\n",
      "  - linkpred_top_predictions.csv\n",
      "\n",
      "Link pred metrics exists: True\n",
      "Embeddings exists: True\n",
      "Top predictions exists: True\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# DISCOVER ARTIFACTS\n",
    "# ============================================================================\n",
    "\n",
    "embed_keywords = [\"embed\", \"node2vec\", \"linkpred\", \"auc\", \"ap\", \"mrr\", \"hits\", \"prediction\"]\n",
    "\n",
    "# Search in analysis directory\n",
    "analysis_files = list(ANALYSIS_DIR.glob(\"*.parquet\")) + list(ANALYSIS_DIR.glob(\"*.json\"))\n",
    "embed_candidates = [\n",
    "    f for f in analysis_files \n",
    "    if any(kw in f.name.lower() for kw in embed_keywords)\n",
    "]\n",
    "\n",
    "# Search in tables directory for predictions\n",
    "table_files = list(TABLES_DIR.glob(\"*linkpred*.csv\")) + list(TABLES_DIR.glob(\"*prediction*.csv\"))\n",
    "\n",
    "print(f\"Found {len(embed_candidates)} embedding/linkpred artifacts in analysis/:\")\n",
    "for ef in sorted(embed_candidates):\n",
    "    print(f\"  - {ef.name}\")\n",
    "\n",
    "print(f\"\\nFound {len(table_files)} prediction tables in tables/:\")\n",
    "for tf in sorted(table_files):\n",
    "    print(f\"  - {tf.name}\")\n",
    "\n",
    "# Primary files\n",
    "linkpred_metrics_file = ANALYSIS_DIR / \"linkpred_metrics.json\"\n",
    "embeddings_file = ANALYSIS_DIR / \"airport_embeddings.parquet\"\n",
    "predictions_file = TABLES_DIR / \"linkpred_top_predictions.csv\"\n",
    "\n",
    "print(f\"\\nLink pred metrics exists: {linkpred_metrics_file.exists()}\")\n",
    "print(f\"Embeddings exists: {embeddings_file.exists()}\")\n",
    "print(f\"Top predictions exists: {predictions_file.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d2c0a2",
   "metadata": {},
   "source": [
    "<a id=\"load-metrics\"></a>\n",
    "## 3. Load and Inspect Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b2127c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LINK PREDICTION METRICS:\n",
      "{\n",
      "  \"baseline_heuristics\": {\n",
      "    \"common_neighbors\": {\n",
      "      \"auc\": 0.8854560491493384,\n",
      "      \"avg_precision\": 0.6501444258106287\n",
      "    },\n",
      "    \"jaccard\": {\n",
      "      \"auc\": 0.7783790170132324,\n",
      "      \"avg_precision\": 0.36629631118299083\n",
      "    },\n",
      "    \"adamic_adar\": {\n",
      "      \"auc\": 0.8867054584120982,\n",
      "      \"avg_precision\": 0.6555001421747944\n",
      "    },\n",
      "    \"preferential_attachment\": {\n",
      "      \"auc\": 0.8913781899810965,\n",
      "      \"avg_precision\": 0.6690358405233544\n",
      "    }\n",
      "  },\n",
      "  \"embedding_classifier\": {\n",
      "    \"auc\": 0.8661507561436673,\n",
      "    \"avg_precision\": 0.6507278844201276\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# LOAD AND INSPECT METRICS\n",
    "# ============================================================================\n",
    "\n",
    "linkpred_metrics = None\n",
    "metrics_flat = {}\n",
    "\n",
    "if linkpred_metrics_file.exists():\n",
    "    with open(linkpred_metrics_file) as f:\n",
    "        linkpred_metrics = json.load(f)\n",
    "    \n",
    "    print(\"LINK PREDICTION METRICS:\")\n",
    "    print(json.dumps(linkpred_metrics, indent=2))\n",
    "    \n",
    "    # Flatten for table output\n",
    "    metrics_flat = flatten_metrics(linkpred_metrics)\n",
    "else:\n",
    "    append_warning(\"linkpred_metrics.json not found\")\n",
    "    print(\"Not available: link prediction metrics not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ec67d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FLATTENED METRICS TABLE:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>baseline_heuristics__common_neighbors__auc</td>\n",
       "      <td>0.885456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>baseline_heuristics__common_neighbors__avg_pre...</td>\n",
       "      <td>0.650144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>baseline_heuristics__jaccard__auc</td>\n",
       "      <td>0.778379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>baseline_heuristics__jaccard__avg_precision</td>\n",
       "      <td>0.366296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>baseline_heuristics__adamic_adar__auc</td>\n",
       "      <td>0.886705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>baseline_heuristics__adamic_adar__avg_precision</td>\n",
       "      <td>0.655500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>baseline_heuristics__preferential_attachment__auc</td>\n",
       "      <td>0.891378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>baseline_heuristics__preferential_attachment__...</td>\n",
       "      <td>0.669036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>embedding_classifier__auc</td>\n",
       "      <td>0.866151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>embedding_classifier__avg_precision</td>\n",
       "      <td>0.650728</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              metric     value\n",
       "0         baseline_heuristics__common_neighbors__auc  0.885456\n",
       "1  baseline_heuristics__common_neighbors__avg_pre...  0.650144\n",
       "2                  baseline_heuristics__jaccard__auc  0.778379\n",
       "3        baseline_heuristics__jaccard__avg_precision  0.366296\n",
       "4              baseline_heuristics__adamic_adar__auc  0.886705\n",
       "5    baseline_heuristics__adamic_adar__avg_precision  0.655500\n",
       "6  baseline_heuristics__preferential_attachment__auc  0.891378\n",
       "7  baseline_heuristics__preferential_attachment__...  0.669036\n",
       "8                          embedding_classifier__auc  0.866151\n",
       "9                embedding_classifier__avg_precision  0.650728"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä KEY PERFORMANCE METRICS:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>baseline_heuristics__common_neighbors__auc</td>\n",
       "      <td>0.885456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>baseline_heuristics__jaccard__auc</td>\n",
       "      <td>0.778379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>baseline_heuristics__adamic_adar__auc</td>\n",
       "      <td>0.886705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>baseline_heuristics__preferential_attachment__auc</td>\n",
       "      <td>0.891378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>embedding_classifier__auc</td>\n",
       "      <td>0.866151</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              metric     value\n",
       "0         baseline_heuristics__common_neighbors__auc  0.885456\n",
       "2                  baseline_heuristics__jaccard__auc  0.778379\n",
       "4              baseline_heuristics__adamic_adar__auc  0.886705\n",
       "6  baseline_heuristics__preferential_attachment__auc  0.891378\n",
       "8                          embedding_classifier__auc  0.866151"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SUMMARIZE METRICS IN TABLE FORM\n",
    "# ============================================================================\n",
    "\n",
    "if len(metrics_flat) > 0:\n",
    "    # Create metrics table\n",
    "    metrics_df = pd.DataFrame([\n",
    "        {\"metric\": k, \"value\": v} for k, v in metrics_flat.items()\n",
    "        if isinstance(v, (int, float))\n",
    "    ])\n",
    "    \n",
    "    print(\"\\nFLATTENED METRICS TABLE:\")\n",
    "    display(metrics_df)\n",
    "    \n",
    "    # Highlight key performance indicators\n",
    "    key_metrics = [\"auc\", \"ap\", \"average_precision\", \"roc_auc\", \"mrr\", \"hits@10\"]\n",
    "    key_rows = metrics_df[metrics_df[\"metric\"].str.lower().str.contains(\"|\".join(key_metrics))]\n",
    "    \n",
    "    if len(key_rows) > 0:\n",
    "        print(\"\\nüìä KEY PERFORMANCE METRICS:\")\n",
    "        display(key_rows)\n",
    "else:\n",
    "    metrics_df = pd.DataFrame()\n",
    "    print(\"Not available: no metrics to display\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1f3a0d",
   "metadata": {},
   "source": [
    "<a id=\"embedding-quality\"></a>\n",
    "## 4. Embedding Quality Analysis\n",
    "\n",
    "Perform basic sanity checks on embeddings if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0557b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (348, 3)\n",
      "Columns: ['vertex_id', 'code', 'embedding']\n",
      "Could not identify embedding dimensions\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# EMBEDDING QUALITY ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "embeddings = None\n",
    "\n",
    "if embeddings_file.exists():\n",
    "    embeddings = safe_load_parquet(embeddings_file)\n",
    "    \n",
    "    if embeddings is not None:\n",
    "        print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "        print(f\"Columns: {embeddings.columns}\")\n",
    "        \n",
    "        # Identify embedding dimension columns (usually numeric, many columns)\n",
    "        numeric_cols = [c for c in embeddings.columns \n",
    "                       if embeddings[c].dtype in [pl.Float64, pl.Float32]]\n",
    "        \n",
    "        if len(numeric_cols) > 5:  # Likely embedding dimensions\n",
    "            print(f\"\\nDetected {len(numeric_cols)} embedding dimensions\")\n",
    "            \n",
    "            # Compute L2 norms\n",
    "            embed_matrix = embeddings.select(numeric_cols).to_numpy()\n",
    "            norms = np.linalg.norm(embed_matrix, axis=1)\n",
    "            \n",
    "            print(f\"\\nEmbedding Norm Statistics:\")\n",
    "            print(f\"  Mean: {norms.mean():.4f}\")\n",
    "            print(f\"  Std: {norms.std():.4f}\")\n",
    "            print(f\"  Min: {norms.min():.4f}\")\n",
    "            print(f\"  Max: {norms.max():.4f}\")\n",
    "            \n",
    "            # Check for degenerate embeddings\n",
    "            zero_norms = (norms < 1e-6).sum()\n",
    "            if zero_norms > 0:\n",
    "                append_warning(f\"{zero_norms} embeddings have near-zero norm\")\n",
    "            \n",
    "            # Plot norm distribution\n",
    "            fig, ax = plt.subplots(figsize=(10, 5))\n",
    "            ax.hist(norms, bins=50, edgecolor=\"white\", alpha=0.8)\n",
    "            ax.set_xlabel(\"Embedding L2 Norm\")\n",
    "            ax.set_ylabel(\"Frequency\")\n",
    "            ax.set_title(f\"Embedding Norm Distribution (dim={len(numeric_cols)})\")\n",
    "            ax.axvline(norms.mean(), color=\"red\", linestyle=\"--\", label=f\"Mean: {norms.mean():.3f}\")\n",
    "            ax.legend()\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            fig_path = FIGURES_REPORT_DIR / f\"{NOTEBOOK_ID}_embedding_norms_distribution.png\"\n",
    "            plt.savefig(fig_path, dpi=150)\n",
    "            plt.show()\n",
    "            print(f\"‚úÖ Saved: {fig_path.name}\")\n",
    "        else:\n",
    "            print(\"Could not identify embedding dimensions\")\n",
    "else:\n",
    "    print(\"Not available: embeddings file not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9f550c",
   "metadata": {},
   "source": [
    "<a id=\"top-predictions\"></a>\n",
    "## 5. Top Predictions Analysis\n",
    "\n",
    "Examine the top predicted new links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5cf569f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top predictions shape: (100, 4)\n",
      "Columns: ['origin', 'dest', 'score', 'rank']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>origin</th>\n",
       "      <th>dest</th>\n",
       "      <th>score</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SFB</td>\n",
       "      <td>PHX</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CLT</td>\n",
       "      <td>PIE</td>\n",
       "      <td>0.999994</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MDW</td>\n",
       "      <td>IND</td>\n",
       "      <td>0.999934</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>STL</td>\n",
       "      <td>JFK</td>\n",
       "      <td>0.999890</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PBI</td>\n",
       "      <td>BNA</td>\n",
       "      <td>0.998135</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>VPS</td>\n",
       "      <td>PDX</td>\n",
       "      <td>0.990990</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AVL</td>\n",
       "      <td>IAD</td>\n",
       "      <td>0.987661</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>HDN</td>\n",
       "      <td>PHX</td>\n",
       "      <td>0.986235</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>FSD</td>\n",
       "      <td>DCA</td>\n",
       "      <td>0.980781</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>BOS</td>\n",
       "      <td>TUL</td>\n",
       "      <td>0.975798</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>PHX</td>\n",
       "      <td>EYW</td>\n",
       "      <td>0.970186</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>PHX</td>\n",
       "      <td>SGF</td>\n",
       "      <td>0.952242</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>BNA</td>\n",
       "      <td>LEX</td>\n",
       "      <td>0.933771</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>SFB</td>\n",
       "      <td>GEG</td>\n",
       "      <td>0.932151</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>GST</td>\n",
       "      <td>KTN</td>\n",
       "      <td>0.926173</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>RIC</td>\n",
       "      <td>MCI</td>\n",
       "      <td>0.909953</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ATW</td>\n",
       "      <td>SAN</td>\n",
       "      <td>0.885167</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>SBN</td>\n",
       "      <td>AUS</td>\n",
       "      <td>0.872348</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>TUS</td>\n",
       "      <td>STL</td>\n",
       "      <td>0.867773</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>PDX</td>\n",
       "      <td>SYR</td>\n",
       "      <td>0.866810</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   origin dest     score  rank\n",
       "0     SFB  PHX  1.000000     1\n",
       "1     CLT  PIE  0.999994     2\n",
       "2     MDW  IND  0.999934     3\n",
       "3     STL  JFK  0.999890     4\n",
       "4     PBI  BNA  0.998135     5\n",
       "5     VPS  PDX  0.990990     6\n",
       "6     AVL  IAD  0.987661     7\n",
       "7     HDN  PHX  0.986235     8\n",
       "8     FSD  DCA  0.980781     9\n",
       "9     BOS  TUL  0.975798    10\n",
       "10    PHX  EYW  0.970186    11\n",
       "11    PHX  SGF  0.952242    12\n",
       "12    BNA  LEX  0.933771    13\n",
       "13    SFB  GEG  0.932151    14\n",
       "14    GST  KTN  0.926173    15\n",
       "15    RIC  MCI  0.909953    16\n",
       "16    ATW  SAN  0.885167    17\n",
       "17    SBN  AUS  0.872348    18\n",
       "18    TUS  STL  0.867773    19\n",
       "19    PDX  SYR  0.866810    20"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# TOP PREDICTIONS ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "predictions = None\n",
    "\n",
    "if predictions_file.exists():\n",
    "    predictions = pd.read_csv(predictions_file)\n",
    "    print(f\"Top predictions shape: {predictions.shape}\")\n",
    "    print(f\"Columns: {list(predictions.columns)}\")\n",
    "    display(predictions.head(20))\n",
    "else:\n",
    "    print(\"Not available: top predictions file not found\")\n",
    "    \n",
    "    # Try alternative locations\n",
    "    for alt_file in TABLES_DIR.glob(\"*predict*.csv\"):\n",
    "        print(f\"Found alternative: {alt_file.name}\")\n",
    "        predictions = pd.read_csv(alt_file)\n",
    "        display(predictions.head(10))\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174e7abe",
   "metadata": {},
   "source": [
    "<a id=\"plausibility\"></a>\n",
    "## 6. Prediction Plausibility\n",
    "\n",
    "Assess whether predicted links are plausible based on network structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b3f3d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction columns: source=origin, target=dest, score=score\n",
      "\n",
      "Most frequent SOURCE airports in predictions:\n",
      "origin\n",
      "PHX    4\n",
      "RIC    3\n",
      "FLL    3\n",
      "BNA    3\n",
      "SFB    2\n",
      "CHS    2\n",
      "XNA    2\n",
      "LWS    2\n",
      "BWI    2\n",
      "VLD    2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Most frequent TARGET airports in predictions:\n",
      "dest\n",
      "MCI    3\n",
      "IND    3\n",
      "CMH    3\n",
      "PHX    2\n",
      "HRL    2\n",
      "BTR    2\n",
      "PBI    2\n",
      "PIE    2\n",
      "CHS    2\n",
      "HHH    2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "üìä PLAUSIBILITY CHECK:\n",
      "   Predictions involving mega-hubs: 14 (14.0%)\n",
      "   ‚úÖ Predictions show structural diversity\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PREDICTION PLAUSIBILITY ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "if predictions is not None and len(predictions) > 0:\n",
    "    # Identify source/target columns\n",
    "    src_col = next((c for c in [\"source\", \"origin\", \"airport_1\", \"src\"] if c in predictions.columns), None)\n",
    "    dst_col = next((c for c in [\"target\", \"dest\", \"airport_2\", \"dst\"] if c in predictions.columns), None)\n",
    "    score_col = next((c for c in [\"score\", \"probability\", \"pred_score\", \"link_prob\"] if c in predictions.columns), None)\n",
    "    \n",
    "    if src_col and dst_col:\n",
    "        print(f\"\\nPrediction columns: source={src_col}, target={dst_col}, score={score_col}\")\n",
    "        \n",
    "        # Check if predictions cluster around mega-hubs\n",
    "        top_sources = predictions[src_col].value_counts().head(10)\n",
    "        top_targets = predictions[dst_col].value_counts().head(10)\n",
    "        \n",
    "        print(\"\\nMost frequent SOURCE airports in predictions:\")\n",
    "        print(top_sources)\n",
    "        \n",
    "        print(\"\\nMost frequent TARGET airports in predictions:\")\n",
    "        print(top_targets)\n",
    "        \n",
    "        # Plausibility assessment\n",
    "        mega_hubs = {\"ATL\", \"ORD\", \"DFW\", \"DEN\", \"LAX\", \"CLT\", \"PHX\", \"IAH\", \"SFO\", \"EWR\"}\n",
    "        hub_predictions = predictions[\n",
    "            predictions[src_col].isin(mega_hubs) | predictions[dst_col].isin(mega_hubs)\n",
    "        ]\n",
    "        hub_rate = len(hub_predictions) / len(predictions) if len(predictions) > 0 else 0\n",
    "        \n",
    "        print(f\"\\nüìä PLAUSIBILITY CHECK:\")\n",
    "        print(f\"   Predictions involving mega-hubs: {len(hub_predictions)} ({hub_rate:.1%})\")\n",
    "        if hub_rate > 0.5:\n",
    "            print(\"   ‚ö†Ô∏è High hub concentration suggests predictions may be trivial\")\n",
    "        else:\n",
    "            print(\"   ‚úÖ Predictions show structural diversity\")\n",
    "    else:\n",
    "        append_warning(f\"Could not identify source/target columns in predictions\")\n",
    "else:\n",
    "    print(\"Not available: no predictions to analyze\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1792376a",
   "metadata": {},
   "source": [
    "<a id=\"interpretation\"></a>\n",
    "## 7. Interpretation\n",
    "\n",
    "### Key Findings (Evidence-Grounded)\n",
    "\n",
    "1. **Overall Link Prediction Performance**: All methods achieve strong AUC scores (0.78‚Äì0.89), indicating good ability to distinguish positive from negative links.\n",
    "\n",
    "2. **Method Comparison (AUC)**:\n",
    "   | Method | AUC | Avg Precision |\n",
    "   |--------|-----|---------------|\n",
    "   | Preferential Attachment | **0.891** | **0.669** |\n",
    "   | Adamic-Adar | 0.887 | 0.656 |\n",
    "   | Common Neighbors | 0.885 | 0.650 |\n",
    "   | Embedding Classifier | 0.866 | 0.651 |\n",
    "   | Jaccard | 0.778 | 0.366 |\n",
    "\n",
    "3. **Key Insight: Simple Heuristics Win**\n",
    "   - Preferential attachment (AUC=0.891) **outperforms** the embedding classifier (AUC=0.866)\n",
    "   - This suggests link formation in the airport network is primarily driven by **degree assortativity** ‚Äî high-traffic airports tend to connect to other high-traffic airports\n",
    "   - The ~2.5% AUC advantage of heuristics over embeddings indicates learned representations don't capture substantially more signal than structural heuristics\n",
    "\n",
    "4. **Prediction Plausibility Assessment**:\n",
    "   - Top 100 predictions analyzed\n",
    "   - **Only 14% involve mega-hubs** (ATL, ORD, DFW, etc.) ‚Äî indicating non-trivial predictions\n",
    "   - Most frequent source: PHX (4), RIC (3), FLL (3), BNA (3)\n",
    "   - Most frequent target: MCI (3), IND (3), CMH (3)\n",
    "   - Predictions suggest potential routes between **secondary airports and regional markets**\n",
    "\n",
    "5. **Top Predicted New Routes** (sample):\n",
    "   | Rank | Origin | Destination | Score |\n",
    "   |------|--------|-------------|-------|\n",
    "   | 1 | SFB (Orlando Sanford) | PHX | 0.9999 |\n",
    "   | 2 | CLT | PIE (St. Pete) | 0.9999 |\n",
    "   | 3 | MDW | IND | 0.9999 |\n",
    "   | 4 | STL | JFK | 0.9999 |\n",
    "   | 5 | PBI (Palm Beach) | BNA | 0.9981 |\n",
    "\n",
    "6. **Embedding Structure**:\n",
    "   - 348 airport embeddings generated\n",
    "   - Stored in array format (not expanded dimensions)\n",
    "   - Embedding norm distribution: **Not computed** (format requires array parsing)\n",
    "\n",
    "### Mechanistic Explanation (Network Science Reasoning)\n",
    "\n",
    "- **Preferential attachment dominance**: The best-performing heuristic (preferential attachment = degree product) reflects the \"rich get richer\" dynamics in airline networks ‚Äî large hubs attract new routes proportionally to their existing connectivity.\n",
    "\n",
    "- **Adamic-Adar and Common Neighbors**: These neighbor-overlap heuristics also perform well, indicating that shared third-party connections predict new links. This aligns with airline alliance dynamics and code-sharing patterns.\n",
    "\n",
    "- **Embedding underperformance**: The node2vec embeddings likely capture local random walk neighborhoods, which may overlap substantially with what simple heuristics already measure. The incremental learning benefit is small.\n",
    "\n",
    "- **Jaccard weakness**: Jaccard normalization (overlap/union) hurts performance because it penalizes high-degree nodes, which are actually MORE likely to form new links in this network.\n",
    "\n",
    "### Alternative Explanations and Confounders\n",
    "\n",
    "1. **Train/test split temporal leakage**: If edges are not strictly time-separated, metrics may be inflated for all methods equally.\n",
    "\n",
    "2. **Class imbalance**: Link prediction has extreme negative/positive imbalance (most node pairs are not connected). High AUC can be achieved even with high false positive rates.\n",
    "\n",
    "3. **Trivial predictions**: Although hub rate is low (14%), predictions still favor well-connected airports ‚Äî additional validation against business plans would be needed.\n",
    "\n",
    "4. **Embedding hyperparameters**: Different node2vec parameters (p, q, walk length) could yield different results.\n",
    "\n",
    "### Sensitivity / Robustness Notes\n",
    "\n",
    "- **Cross-method consistency**: All methods except Jaccard show strong AUC (0.86‚Äì0.89), suggesting results are robust across approaches.\n",
    "- **Threshold sensitivity**: Not available ‚Äî would require precision-recall curves at different thresholds.\n",
    "- **Temporal validation**: Not available ‚Äî results use a single train/test split.\n",
    "\n",
    "### Evidence Links\n",
    "\n",
    "| Artifact Type | Path |\n",
    "|---------------|------|\n",
    "| **Table** | `results/tables/report/nb07_linkpred_metrics_flat.csv` |\n",
    "| **Table** | `results/tables/report/nb07_top_predictions_annotated.csv` |\n",
    "| **Source Data** | `results/analysis/linkpred_metrics.json` |\n",
    "| **Embeddings** | `results/analysis/airport_embeddings.parquet` |\n",
    "| **Predictions** | `results/tables/linkpred_top_predictions.csv` |\n",
    "\n",
    "### Implications\n",
    "\n",
    "**Operational implications:**\n",
    "- Simple degree-based heuristics are sufficient for route prediction ‚Äî no need for complex ML models\n",
    "- Top predictions identify potential market opportunities (SFB-PHX, CLT-PIE, MDW-IND)\n",
    "- Secondary airport connectivity gaps (not mega-hub focused) suggest underserved regional markets\n",
    "\n",
    "**Research implications:**\n",
    "- Preferential attachment remains the dominant link formation mechanism in mature transportation networks\n",
    "- Embeddings may add more value in networks with less degree heterogeneity\n",
    "- Future work should test time-stratified evaluation and incorporate exogenous features (passenger demand, competition)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375982f1",
   "metadata": {},
   "source": [
    "<a id=\"write-outputs\"></a>\n",
    "## 8. Write Report Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62a4659d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Wrote: c:\\Users\\aster\\projects-source\\network_science_VTSL\\results\\tables\\report\\nb07_linkpred_metrics_flat.csv\n",
      "‚úÖ Wrote: c:\\Users\\aster\\projects-source\\network_science_VTSL\\results\\tables\\report\\nb07_top_predictions_annotated.csv\n",
      "\n",
      "üìã All nb07 outputs written.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# WRITE REPORT OUTPUTS\n",
    "# ============================================================================\n",
    "\n",
    "# Write flattened metrics\n",
    "if len(metrics_df) > 0:\n",
    "    metrics_path = TABLES_REPORT_DIR / f\"{NOTEBOOK_ID}_linkpred_metrics_flat.csv\"\n",
    "    metrics_df.to_csv(metrics_path, index=False)\n",
    "    print(f\"‚úÖ Wrote: {metrics_path}\")\n",
    "\n",
    "# Write annotated predictions\n",
    "if predictions is not None and len(predictions) > 0:\n",
    "    pred_path = TABLES_REPORT_DIR / f\"{NOTEBOOK_ID}_top_predictions_annotated.csv\"\n",
    "    predictions.to_csv(pred_path, index=False)\n",
    "    print(f\"‚úÖ Wrote: {pred_path}\")\n",
    "\n",
    "print(f\"\\nüìã All {NOTEBOOK_ID} outputs written.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c23969",
   "metadata": {},
   "source": [
    "<a id=\"reproducibility\"></a>\n",
    "## 9. Reproducibility Notes\n",
    "\n",
    "### Run Provenance\n",
    "| Field | Value |\n",
    "|-------|-------|\n",
    "| **Pipeline Script** | `08_run_embeddings_linkpred.py` |\n",
    "| **Embedding Method** | node2vec (inferred from artifact naming) |\n",
    "| **Number of Airports** | 348 |\n",
    "| **Top Predictions Returned** | 100 |\n",
    "\n",
    "### Methods Evaluated\n",
    "| Method | Type | Description |\n",
    "|--------|------|-------------|\n",
    "| Common Neighbors | Heuristic | Count of shared neighbors between node pairs |\n",
    "| Jaccard | Heuristic | Common neighbors / union of neighbors |\n",
    "| Adamic-Adar | Heuristic | Weighted common neighbors (inverse log degree) |\n",
    "| Preferential Attachment | Heuristic | Product of node degrees |\n",
    "| Embedding Classifier | ML | Dot product of learned node embeddings |\n",
    "\n",
    "### Input Files Consumed\n",
    "| File | Status | Description |\n",
    "|------|--------|-------------|\n",
    "| `results/analysis/linkpred_metrics.json` | ‚úÖ Present | Performance metrics (AUC, AP) |\n",
    "| `results/analysis/airport_embeddings.parquet` | ‚úÖ Present | 348 √ó 3 columns (vertex_id, code, embedding array) |\n",
    "| `results/tables/linkpred_top_predictions.csv` | ‚úÖ Present | 100 top predicted links with scores |\n",
    "\n",
    "### Assumptions Made\n",
    "1. Embeddings were trained on the airport-level network (not flight network)\n",
    "2. Link prediction uses a temporal train/test split (edges from later period as test set)\n",
    "3. Negative sampling used random non-edges for evaluation\n",
    "4. All metrics computed on the same hold-out set for fair comparison\n",
    "\n",
    "### Metrics Definitions\n",
    "| Metric | Definition | Interpretation |\n",
    "|--------|------------|----------------|\n",
    "| **AUC** | Area under ROC curve | Probability that a random positive edge ranks higher than a random negative |\n",
    "| **Avg Precision** | Area under precision-recall curve | Quality of ranking, weighted by precision at each threshold |\n",
    "\n",
    "### Key Metrics Summary\n",
    "| Method | AUC | Avg Precision |\n",
    "|--------|-----|---------------|\n",
    "| Preferential Attachment | 0.891 | 0.669 |\n",
    "| Adamic-Adar | 0.887 | 0.656 |\n",
    "| Common Neighbors | 0.885 | 0.650 |\n",
    "| Embedding Classifier | 0.866 | 0.651 |\n",
    "| Jaccard | 0.778 | 0.366 |\n",
    "\n",
    "### Outputs Generated\n",
    "| Artifact | Path | Description |\n",
    "|----------|------|-------------|\n",
    "| Metrics Table | `results/tables/report/nb07_linkpred_metrics_flat.csv` | Flattened metrics for all methods |\n",
    "| Annotated Predictions | `results/tables/report/nb07_top_predictions_annotated.csv` | Top 100 predicted new routes |\n",
    "\n",
    "### Outputs Not Generated\n",
    "| Planned Artifact | Reason |\n",
    "|------------------|--------|\n",
    "| Embedding Norms Distribution | Embeddings stored as arrays in single column, not expanded dimensions |\n",
    "\n",
    "### Plausibility Summary\n",
    "- **Hub concentration in predictions**: 14% (14/100 predictions involve mega-hubs)\n",
    "- **Interpretation**: Predictions show structural diversity, targeting secondary markets rather than trivially predicting hub connections\n",
    "\n",
    "### Notebook Execution\n",
    "- **Execution Date**: 2025-12-27\n",
    "- **All cells executed**: Yes\n",
    "- **Warnings logged**: None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "network_science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
