{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfcc7f0b",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. [Setup and Imports](#setup)\n",
    "2. [Inventory All Report Artifacts](#inventory)\n",
    "3. [Master Evidence Index](#evidence-index)\n",
    "4. [Cross-Domain Synthesis](#cross-domain)\n",
    "5. [Centrality â†” Delay Correlation](#centrality-delay)\n",
    "6. [Robustness â†” Hub Dependence](#robustness-hub)\n",
    "7. [Integrated Narrative](#narrative)\n",
    "8. [Write Report Outputs](#write-outputs)\n",
    "9. [Reproducibility Notes](#reproducibility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e617f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SETUP AND IMPORTS\n",
    "# ============================================================================\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Project paths\n",
    "REPO_ROOT = Path.cwd().parent.parent\n",
    "RESULTS_DIR = REPO_ROOT / \"results\"\n",
    "ANALYSIS_DIR = RESULTS_DIR / \"analysis\"\n",
    "BUSINESS_DIR = RESULTS_DIR / \"business\"\n",
    "NETWORKS_DIR = RESULTS_DIR / \"networks\"\n",
    "TABLES_REPORT_DIR = RESULTS_DIR / \"tables\" / \"report\"\n",
    "FIGURES_REPORT_DIR = RESULTS_DIR / \"figures\" / \"report\"\n",
    "LOGS_DIR = RESULTS_DIR / \"logs\"\n",
    "WARNINGS_LOG = TABLES_REPORT_DIR / \"_warnings.log\"\n",
    "\n",
    "# Notebook identity\n",
    "NOTEBOOK_ID = \"nb09\"\n",
    "NOTEBOOK_NAME = \"synthesis__integrated_findings\"\n",
    "\n",
    "# Plotting settings\n",
    "plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Ensure output directories exist\n",
    "TABLES_REPORT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "FIGURES_REPORT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Results dir exists: {RESULTS_DIR.exists()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285f3cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# HELPER FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def append_warning(message: str, notebook_id: str = NOTEBOOK_ID):\n",
    "    \"\"\"Append a warning to the consolidated warnings log.\"\"\"\n",
    "    timestamp = datetime.now().isoformat()\n",
    "    with open(WARNINGS_LOG, \"a\") as f:\n",
    "        f.write(f\"[{timestamp}] [{notebook_id}] {message}\\n\")\n",
    "    print(f\"WARNING: {message}\")\n",
    "\n",
    "def safe_load_parquet(path: Path) -> pl.DataFrame | None:\n",
    "    \"\"\"Safely load a parquet file, returning None if it fails.\"\"\"\n",
    "    try:\n",
    "        return pl.read_parquet(path)\n",
    "    except Exception as e:\n",
    "        append_warning(f\"Failed to load {path.name}: {e}\")\n",
    "        return None\n",
    "\n",
    "def safe_load_json(path: Path) -> dict | None:\n",
    "    \"\"\"Safely load a JSON file, returning None if it fails.\"\"\"\n",
    "    try:\n",
    "        with open(path) as f:\n",
    "            return json.load(f)\n",
    "    except Exception as e:\n",
    "        append_warning(f\"Failed to load {path.name}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4930cc",
   "metadata": {},
   "source": [
    "<a id=\"inventory\"></a>\n",
    "## 2. Inventory All Report Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55b7f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# INVENTORY ALL REPORT ARTIFACTS\n",
    "# ============================================================================\n",
    "\n",
    "# Collect all artifacts\n",
    "all_artifacts = []\n",
    "\n",
    "# Tables\n",
    "for tbl in TABLES_REPORT_DIR.glob(\"*.csv\"):\n",
    "    nb_id = tbl.stem.split(\"_\")[0] if \"_\" in tbl.stem else \"misc\"\n",
    "    all_artifacts.append({\n",
    "        \"file\": tbl.name,\n",
    "        \"type\": \"table\",\n",
    "        \"notebook\": nb_id,\n",
    "        \"path\": str(tbl.relative_to(RESULTS_DIR))\n",
    "    })\n",
    "\n",
    "# Figures\n",
    "for fig in FIGURES_REPORT_DIR.glob(\"*.png\"):\n",
    "    nb_id = fig.stem.split(\"_\")[0] if \"_\" in fig.stem else \"misc\"\n",
    "    all_artifacts.append({\n",
    "        \"file\": fig.name,\n",
    "        \"type\": \"figure\",\n",
    "        \"notebook\": nb_id,\n",
    "        \"path\": str(fig.relative_to(RESULTS_DIR))\n",
    "    })\n",
    "\n",
    "# Pipeline artifacts (analysis)\n",
    "for art in ANALYSIS_DIR.glob(\"*\"):\n",
    "    all_artifacts.append({\n",
    "        \"file\": art.name,\n",
    "        \"type\": \"pipeline\",\n",
    "        \"notebook\": \"pipeline\",\n",
    "        \"path\": str(art.relative_to(RESULTS_DIR))\n",
    "    })\n",
    "\n",
    "artifact_df = pd.DataFrame(all_artifacts)\n",
    "print(f\"Total artifacts cataloged: {len(artifact_df)}\")\n",
    "\n",
    "# Summary by notebook\n",
    "print(\"\\nArtifacts by source:\")\n",
    "display(artifact_df.groupby([\"notebook\", \"type\"]).size().unstack(fill_value=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ada4db5",
   "metadata": {},
   "source": [
    "<a id=\"evidence-index\"></a>\n",
    "## 3. Master Evidence Index\n",
    "\n",
    "Map each research question to the artifacts that answer it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf4ab62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MASTER EVIDENCE INDEX\n",
    "# ============================================================================\n",
    "\n",
    "# Define research questions and their evidence mappings\n",
    "evidence_index = [\n",
    "    {\n",
    "        \"research_question\": \"What is the basic network structure?\",\n",
    "        \"domain\": \"structure\",\n",
    "        \"primary_artifact\": \"nb02_network_stats.csv\",\n",
    "        \"supporting_artifacts\": \"nb02_top_routes.csv, nb02_degree_distribution.png\",\n",
    "        \"notebook\": \"nb02\"\n",
    "    },\n",
    "    {\n",
    "        \"research_question\": \"Which airports are most central?\",\n",
    "        \"domain\": \"centrality\",\n",
    "        \"primary_artifact\": \"nb03_centrality_topK.csv\",\n",
    "        \"supporting_artifacts\": \"nb03_centrality_distributions.png\",\n",
    "        \"notebook\": \"nb03\"\n",
    "    },\n",
    "    {\n",
    "        \"research_question\": \"What community structure exists?\",\n",
    "        \"domain\": \"communities\",\n",
    "        \"primary_artifact\": \"nb04_community_sizes.csv\",\n",
    "        \"supporting_artifacts\": \"nb04_community_size_distribution.png\",\n",
    "        \"notebook\": \"nb04\"\n",
    "    },\n",
    "    {\n",
    "        \"research_question\": \"How robust is the network to failures?\",\n",
    "        \"domain\": \"robustness\",\n",
    "        \"primary_artifact\": \"nb05_robustness_metrics.csv\",\n",
    "        \"supporting_artifacts\": \"nb05_robustness_curves.png\",\n",
    "        \"notebook\": \"nb05\"\n",
    "    },\n",
    "    {\n",
    "        \"research_question\": \"How do delays propagate through the network?\",\n",
    "        \"domain\": \"dynamics\",\n",
    "        \"primary_artifact\": \"nb06_superspreaders.csv\",\n",
    "        \"supporting_artifacts\": \"nb06_cascade_distribution.png\",\n",
    "        \"notebook\": \"nb06\"\n",
    "    },\n",
    "    {\n",
    "        \"research_question\": \"Can we predict missing links?\",\n",
    "        \"domain\": \"prediction\",\n",
    "        \"primary_artifact\": \"nb07_linkpred_summary.csv\",\n",
    "        \"supporting_artifacts\": \"nb07_linkpred_metrics.png\",\n",
    "        \"notebook\": \"nb07\"\n",
    "    },\n",
    "    {\n",
    "        \"research_question\": \"What are the business implications?\",\n",
    "        \"domain\": \"business\",\n",
    "        \"primary_artifact\": \"nb08_airline_kpi_summary.csv\",\n",
    "        \"supporting_artifacts\": \"nb08_hub_concentration.png\",\n",
    "        \"notebook\": \"nb08\"\n",
    "    }\n",
    "]\n",
    "\n",
    "evidence_df = pd.DataFrame(evidence_index)\n",
    "\n",
    "# Check which artifacts actually exist\n",
    "def check_exists(artifact_name):\n",
    "    return (TABLES_REPORT_DIR / artifact_name).exists() or \\\n",
    "           (FIGURES_REPORT_DIR / artifact_name).exists()\n",
    "\n",
    "evidence_df[\"primary_exists\"] = evidence_df[\"primary_artifact\"].apply(check_exists)\n",
    "\n",
    "print(\"Master Evidence Index:\")\n",
    "display(evidence_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cfe9a7",
   "metadata": {},
   "source": [
    "<a id=\"cross-domain\"></a>\n",
    "## 4. Cross-Domain Synthesis\n",
    "\n",
    "Identify patterns that span multiple analysis domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1c0f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CROSS-DOMAIN SYNTHESIS\n",
    "# ============================================================================\n",
    "\n",
    "# Load centrality data\n",
    "centrality_path = ANALYSIS_DIR / \"airport_centrality.parquet\"\n",
    "centrality_df = safe_load_parquet(centrality_path)\n",
    "\n",
    "# Load delay propagation data\n",
    "delay_path = ANALYSIS_DIR / \"delay_cascades.parquet\"\n",
    "delay_df = safe_load_parquet(delay_path)\n",
    "\n",
    "# Load robustness data\n",
    "robustness_path = ANALYSIS_DIR / \"robustness_curves.parquet\"\n",
    "robustness_df = safe_load_parquet(robustness_path)\n",
    "\n",
    "# Summary\n",
    "cross_domain_insights = []\n",
    "\n",
    "if centrality_df is not None:\n",
    "    cross_domain_insights.append({\n",
    "        \"domain_pair\": \"structure-centrality\",\n",
    "        \"finding\": f\"Centrality computed for {len(centrality_df)} airports\",\n",
    "        \"evidence\": \"airport_centrality.parquet\"\n",
    "    })\n",
    "\n",
    "if delay_df is not None:\n",
    "    cross_domain_insights.append({\n",
    "        \"domain_pair\": \"centrality-dynamics\",\n",
    "        \"finding\": f\"Delay cascades recorded for {len(delay_df)} events\",\n",
    "        \"evidence\": \"delay_cascades.parquet\"\n",
    "    })\n",
    "\n",
    "if robustness_df is not None:\n",
    "    cross_domain_insights.append({\n",
    "        \"domain_pair\": \"structure-robustness\",\n",
    "        \"finding\": f\"Robustness curves with {len(robustness_df)} data points\",\n",
    "        \"evidence\": \"robustness_curves.parquet\"\n",
    "    })\n",
    "\n",
    "if cross_domain_insights:\n",
    "    synthesis_df = pd.DataFrame(cross_domain_insights)\n",
    "    display(synthesis_df)\n",
    "else:\n",
    "    print(\"Not available: insufficient data for cross-domain synthesis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bac41c",
   "metadata": {},
   "source": [
    "<a id=\"centrality-delay\"></a>\n",
    "## 5. Centrality â†” Delay Correlation\n",
    "\n",
    "Do high-centrality airports amplify delay propagation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e574534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CENTRALITY â†” DELAY CORRELATION\n",
    "# ============================================================================\n",
    "\n",
    "if centrality_df is not None and delay_df is not None:\n",
    "    # Convert to pandas for analysis\n",
    "    cent_pd = centrality_df.to_pandas()\n",
    "    delay_pd = delay_df.to_pandas()\n",
    "    \n",
    "    # Find airport ID columns\n",
    "    cent_airport = next((c for c in [\"airport\", \"AIRPORT\", \"node\"] if c in cent_pd.columns), None)\n",
    "    delay_airport = next((c for c in [\"origin\", \"ORIGIN\", \"airport\", \"seed_airport\"] if c in delay_pd.columns), None)\n",
    "    \n",
    "    if cent_airport and delay_airport:\n",
    "        # Aggregate delay data by airport\n",
    "        delay_agg = delay_pd.groupby(delay_airport).agg({\n",
    "            c: \"mean\" for c in delay_pd.select_dtypes(include=[np.number]).columns\n",
    "        }).reset_index()\n",
    "        delay_agg.columns = [delay_airport] + [f\"mean_{c}\" if c != delay_airport else c for c in delay_agg.columns[1:]]\n",
    "        \n",
    "        # Merge\n",
    "        merged = cent_pd.merge(delay_agg, left_on=cent_airport, right_on=delay_airport, how=\"inner\")\n",
    "        \n",
    "        # Find centrality columns\n",
    "        cent_cols = [c for c in cent_pd.columns if \"degree\" in c.lower() or \"between\" in c.lower() \n",
    "                    or \"closeness\" in c.lower() or \"pagerank\" in c.lower()]\n",
    "        \n",
    "        # Find delay/cascade columns\n",
    "        cascade_cols = [c for c in merged.columns if \"mean_\" in c and c != delay_airport]\n",
    "        \n",
    "        if cent_cols and cascade_cols:\n",
    "            # Compute correlation matrix\n",
    "            corr_cols = cent_cols[:4] + cascade_cols[:4]  # Limit to avoid clutter\n",
    "            corr_matrix = merged[corr_cols].corr()\n",
    "            \n",
    "            fig, ax = plt.subplots(figsize=(10, 8))\n",
    "            sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap=\"RdBu_r\", center=0, ax=ax)\n",
    "            ax.set_title(\"Centrality â†” Delay Cascade Correlation\")\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            fig_path = FIGURES_REPORT_DIR / f\"{NOTEBOOK_ID}_centrality_delay_heatmap.png\"\n",
    "            plt.savefig(fig_path, dpi=150)\n",
    "            plt.show()\n",
    "            print(f\"âœ… Saved: {fig_path.name}\")\n",
    "        else:\n",
    "            print(f\"Could not find suitable columns. Centrality: {cent_cols}, Cascade: {cascade_cols}\")\n",
    "    else:\n",
    "        print(f\"Could not find airport columns. Centrality: {cent_airport}, Delay: {delay_airport}\")\n",
    "else:\n",
    "    print(\"Not available: need both centrality and delay data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f30c38",
   "metadata": {},
   "source": [
    "<a id=\"robustness-hub\"></a>\n",
    "## 6. Robustness â†” Hub Dependence\n",
    "\n",
    "Does hub-and-spoke topology create fragility?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f5ed00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ROBUSTNESS â†” HUB DEPENDENCE\n",
    "# ============================================================================\n",
    "\n",
    "robustness_summary_path = ANALYSIS_DIR / \"robustness_summary.json\"\n",
    "robustness_summary = safe_load_json(robustness_summary_path)\n",
    "\n",
    "if robustness_summary:\n",
    "    print(\"Robustness Summary:\")\n",
    "    print(json.dumps(robustness_summary, indent=2, default=str))\n",
    "    \n",
    "    # Extract key metrics\n",
    "    if \"attack_types\" in robustness_summary or \"strategies\" in robustness_summary:\n",
    "        strategies = robustness_summary.get(\"attack_types\", robustness_summary.get(\"strategies\", {}))\n",
    "        \n",
    "        if isinstance(strategies, dict):\n",
    "            strategy_df = pd.DataFrame([\n",
    "                {\"strategy\": k, **v} if isinstance(v, dict) else {\"strategy\": k, \"value\": v}\n",
    "                for k, v in strategies.items()\n",
    "            ])\n",
    "            \n",
    "            display(strategy_df)\n",
    "            \n",
    "            # Plot if we have AUC or critical point data\n",
    "            if \"auc\" in strategy_df.columns or \"critical_fraction\" in strategy_df.columns:\n",
    "                metric = \"auc\" if \"auc\" in strategy_df.columns else \"critical_fraction\"\n",
    "                \n",
    "                fig, ax = plt.subplots(figsize=(10, 6))\n",
    "                bars = ax.bar(strategy_df[\"strategy\"], strategy_df[metric])\n",
    "                ax.set_ylabel(metric.replace(\"_\", \" \").title())\n",
    "                ax.set_title(\"Robustness by Attack Strategy\")\n",
    "                plt.xticks(rotation=45, ha=\"right\")\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                fig_path = FIGURES_REPORT_DIR / f\"{NOTEBOOK_ID}_robustness_hub_dependence.png\"\n",
    "                plt.savefig(fig_path, dpi=150)\n",
    "                plt.show()\n",
    "                print(f\"âœ… Saved: {fig_path.name}\")\n",
    "else:\n",
    "    print(\"Not available: robustness summary not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e7db98",
   "metadata": {},
   "source": [
    "<a id=\"narrative\"></a>\n",
    "## 7. Integrated Narrative\n",
    "\n",
    "### Cross-Cutting Synthesis\n",
    "\n",
    "*(Populated after running cells above)*\n",
    "\n",
    "#### Structure â†’ Centrality â†’ Dynamics\n",
    "- The airport network exhibits [scale-free / small-world / ?] properties\n",
    "- High-degree hubs dominate multiple centrality measures\n",
    "- These hubs also appear as delay superspreaders\n",
    "\n",
    "#### Robustness Implications\n",
    "- Targeted attacks on high-degree nodes cause faster fragmentation\n",
    "- This hub dependence creates systemic risk\n",
    "\n",
    "#### Business Interpretation\n",
    "- Airlines with concentrated hub strategies may face amplified disruption costs\n",
    "- Trade-off between operational efficiency and network resilience\n",
    "\n",
    "### Evidence Summary Table\n",
    "\n",
    "| Claim | Evidence Artifact | Notebook |\n",
    "|-------|-------------------|----------|\n",
    "| Hub-and-spoke topology | degree distribution | nb02 |\n",
    "| Centrality correlates with delay | heatmap | nb09 |\n",
    "| Targeted attacks fragment faster | robustness curves | nb05 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e60eb8",
   "metadata": {},
   "source": [
    "<a id=\"write-outputs\"></a>\n",
    "## 8. Write Report Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08f4279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# WRITE REPORT OUTPUTS\n",
    "# ============================================================================\n",
    "\n",
    "# Write master evidence index\n",
    "evidence_path = TABLES_REPORT_DIR / f\"{NOTEBOOK_ID}_master_evidence_index.csv\"\n",
    "evidence_df.to_csv(evidence_path, index=False)\n",
    "print(f\"âœ… Wrote: {evidence_path}\")\n",
    "\n",
    "# Write cross-domain synthesis\n",
    "if cross_domain_insights:\n",
    "    synthesis_path = TABLES_REPORT_DIR / f\"{NOTEBOOK_ID}_cross_domain_synthesis.csv\"\n",
    "    synthesis_df.to_csv(synthesis_path, index=False)\n",
    "    print(f\"âœ… Wrote: {synthesis_path}\")\n",
    "\n",
    "print(f\"\\nðŸ“‹ All {NOTEBOOK_ID} outputs written.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926f7432",
   "metadata": {},
   "source": [
    "<a id=\"reproducibility\"></a>\n",
    "## 9. Reproducibility Notes\n",
    "\n",
    "### Input Files Consumed\n",
    "- All `results/tables/report/nb*.csv` files\n",
    "- All `results/figures/report/nb*.png` files\n",
    "- `results/analysis/airport_centrality.parquet`\n",
    "- `results/analysis/delay_cascades.parquet`\n",
    "- `results/analysis/robustness_summary.json`\n",
    "\n",
    "### Assumptions Made\n",
    "1. Prior notebooks have been run and generated their outputs\n",
    "2. Evidence index maps research questions to artifacts\n",
    "3. Cross-domain analysis uses inner joins (airports must appear in all datasets)\n",
    "\n",
    "### Outputs Generated\n",
    "| Artifact | Path |\n",
    "|----------|------|\n",
    "| Master Evidence Index | `results/tables/report/nb09_master_evidence_index.csv` |\n",
    "| Cross-Domain Synthesis | `results/tables/report/nb09_cross_domain_synthesis.csv` |\n",
    "| Centrality-Delay Heatmap | `results/figures/report/nb09_centrality_delay_heatmap.png` |\n",
    "| Robustness-Hub Dependence | `results/figures/report/nb09_robustness_hub_dependence.png` |"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
