{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b050876c",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. [Setup and Imports](#setup)\n",
    "2. [Discover Robustness Artifacts](#discover)\n",
    "3. [Load and Inspect Robustness Data](#load)\n",
    "4. [Plot Robustness Curves](#curves)\n",
    "5. [Compute Summary Metrics](#metrics)\n",
    "6. [Critical Nodes Analysis](#critical)\n",
    "7. [Interpretation](#interpretation)\n",
    "8. [Write Report Outputs](#write-outputs)\n",
    "9. [Reproducibility Notes](#reproducibility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655406eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SETUP AND IMPORTS\n",
    "# ============================================================================\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Project paths\n",
    "REPO_ROOT = Path.cwd().parent.parent\n",
    "RESULTS_DIR = REPO_ROOT / \"results\"\n",
    "ANALYSIS_DIR = RESULTS_DIR / \"analysis\"\n",
    "TABLES_REPORT_DIR = RESULTS_DIR / \"tables\" / \"report\"\n",
    "FIGURES_REPORT_DIR = RESULTS_DIR / \"figures\" / \"report\"\n",
    "WARNINGS_LOG = TABLES_REPORT_DIR / \"_warnings.log\"\n",
    "\n",
    "# Notebook identity\n",
    "NOTEBOOK_ID = \"nb05\"\n",
    "NOTEBOOK_NAME = \"robustness__percolation_and_hub_dependence\"\n",
    "\n",
    "# Plotting settings\n",
    "plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Ensure output directories exist\n",
    "TABLES_REPORT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "FIGURES_REPORT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Analysis dir exists: {ANALYSIS_DIR.exists()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4d0995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# HELPER FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def append_warning(message: str, notebook_id: str = NOTEBOOK_ID):\n",
    "    \"\"\"Append a warning to the consolidated warnings log.\"\"\"\n",
    "    timestamp = datetime.now().isoformat()\n",
    "    with open(WARNINGS_LOG, \"a\") as f:\n",
    "        f.write(f\"[{timestamp}] [{notebook_id}] {message}\\n\")\n",
    "    print(f\"WARNING: {message}\")\n",
    "\n",
    "def safe_load_parquet(path: Path) -> pl.DataFrame | None:\n",
    "    \"\"\"Safely load a parquet file, returning None if it fails.\"\"\"\n",
    "    try:\n",
    "        return pl.read_parquet(path)\n",
    "    except Exception as e:\n",
    "        append_warning(f\"Failed to load {path.name}: {e}\")\n",
    "        return None\n",
    "\n",
    "def compute_auc(x: np.ndarray, y: np.ndarray) -> float:\n",
    "    \"\"\"Compute area under curve using trapezoidal rule.\"\"\"\n",
    "    # Ensure sorted by x\n",
    "    sort_idx = np.argsort(x)\n",
    "    x_sorted = x[sort_idx]\n",
    "    y_sorted = y[sort_idx]\n",
    "    return np.trapz(y_sorted, x_sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a584086",
   "metadata": {},
   "source": [
    "<a id=\"discover\"></a>\n",
    "## 2. Discover Robustness Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1935f640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DISCOVER ROBUSTNESS ARTIFACTS\n",
    "# ============================================================================\n",
    "\n",
    "robustness_keywords = [\"robust\", \"percol\", \"attack\", \"targeted\", \"random\", \"giant\", \"lcc\"]\n",
    "\n",
    "# Search in analysis directory\n",
    "analysis_files = list(ANALYSIS_DIR.glob(\"*.parquet\")) + list(ANALYSIS_DIR.glob(\"*.csv\")) + list(ANALYSIS_DIR.glob(\"*.json\"))\n",
    "robustness_candidates = [\n",
    "    f for f in analysis_files \n",
    "    if any(kw in f.name.lower() for kw in robustness_keywords)\n",
    "]\n",
    "\n",
    "print(f\"Found {len(robustness_candidates)} robustness-related artifacts:\")\n",
    "for rf in sorted(robustness_candidates):\n",
    "    print(f\"  - {rf.name}\")\n",
    "\n",
    "# Look for primary robustness curves file\n",
    "robustness_curves_file = ANALYSIS_DIR / \"robustness_curves.parquet\"\n",
    "robustness_summary_file = ANALYSIS_DIR / \"robustness_summary.json\"\n",
    "\n",
    "print(f\"\\nRobustness curves exists: {robustness_curves_file.exists()}\")\n",
    "print(f\"Robustness summary exists: {robustness_summary_file.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ba5807",
   "metadata": {},
   "source": [
    "<a id=\"load\"></a>\n",
    "## 3. Load and Inspect Robustness Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4283bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# LOAD AND INSPECT ROBUSTNESS DATA\n",
    "# ============================================================================\n",
    "\n",
    "robustness_curves = None\n",
    "robustness_summary = None\n",
    "\n",
    "# Load curves\n",
    "if robustness_curves_file.exists():\n",
    "    robustness_curves = safe_load_parquet(robustness_curves_file)\n",
    "    if robustness_curves is not None:\n",
    "        print(f\"Robustness curves shape: {robustness_curves.shape}\")\n",
    "        print(f\"Columns: {robustness_curves.columns}\")\n",
    "        display(robustness_curves.head(10).to_pandas())\n",
    "else:\n",
    "    append_warning(\"robustness_curves.parquet not found\")\n",
    "\n",
    "# Load summary\n",
    "if robustness_summary_file.exists():\n",
    "    with open(robustness_summary_file) as f:\n",
    "        robustness_summary = json.load(f)\n",
    "    print(f\"\\nRobustness summary keys: {list(robustness_summary.keys())}\")\n",
    "else:\n",
    "    print(\"\\nRobustness summary not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1464767b",
   "metadata": {},
   "source": [
    "<a id=\"curves\"></a>\n",
    "## 4. Plot Robustness Curves\n",
    "\n",
    "Visualize network fragmentation under different attack scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07baa9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PLOT ROBUSTNESS CURVES\n",
    "# ============================================================================\n",
    "\n",
    "if robustness_curves is not None:\n",
    "    # Identify columns\n",
    "    x_col = next((c for c in [\"fraction_removed\", \"frac_removed\", \"step\", \"k\"] \n",
    "                  if c in robustness_curves.columns), None)\n",
    "    y_col = next((c for c in [\"lcc_fraction\", \"giant_fraction\", \"lcc_size\", \"giant\"] \n",
    "                  if c in robustness_curves.columns), None)\n",
    "    scenario_col = next((c for c in [\"scenario\", \"strategy\", \"attack_type\", \"method\"] \n",
    "                         if c in robustness_curves.columns), None)\n",
    "    \n",
    "    print(f\"X-axis: {x_col}, Y-axis: {y_col}, Scenario: {scenario_col}\")\n",
    "    \n",
    "    if x_col and y_col:\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        \n",
    "        if scenario_col:\n",
    "            # Plot by scenario\n",
    "            scenarios = robustness_curves[scenario_col].unique().to_list()\n",
    "            colors = sns.color_palette(\"husl\", len(scenarios))\n",
    "            \n",
    "            for scenario, color in zip(sorted(scenarios), colors):\n",
    "                subset = robustness_curves.filter(pl.col(scenario_col) == scenario).to_pandas()\n",
    "                ax.plot(subset[x_col], subset[y_col], \n",
    "                        label=scenario, color=color, linewidth=2, marker='o', markersize=3)\n",
    "            ax.legend(title=\"Attack Scenario\")\n",
    "        else:\n",
    "            # Single curve\n",
    "            df = robustness_curves.to_pandas()\n",
    "            ax.plot(df[x_col], df[y_col], linewidth=2, marker='o', markersize=3)\n",
    "        \n",
    "        ax.set_xlabel(\"Fraction of Nodes Removed\")\n",
    "        ax.set_ylabel(\"Largest Connected Component Fraction\")\n",
    "        ax.set_title(\"Network Robustness: Random vs Targeted Attacks\")\n",
    "        ax.set_xlim(0, 1)\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        fig_path = FIGURES_REPORT_DIR / f\"{NOTEBOOK_ID}_robustness_curves.png\"\n",
    "        plt.savefig(fig_path, dpi=150)\n",
    "        plt.show()\n",
    "        print(f\"âœ… Saved: {fig_path.name}\")\n",
    "    else:\n",
    "        append_warning(f\"Could not identify x ({x_col}) or y ({y_col}) columns for robustness curves\")\n",
    "else:\n",
    "    print(\"Not available: robustness curves data not loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbf16f3",
   "metadata": {},
   "source": [
    "<a id=\"metrics\"></a>\n",
    "## 5. Compute Summary Metrics\n",
    "\n",
    "Derive robustness metrics like AUC, critical drop point, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204de42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# COMPUTE SUMMARY METRICS\n",
    "# ============================================================================\n",
    "\n",
    "summary_metrics = []\n",
    "\n",
    "if robustness_curves is not None and x_col and y_col:\n",
    "    if scenario_col:\n",
    "        scenarios = robustness_curves[scenario_col].unique().to_list()\n",
    "        \n",
    "        for scenario in scenarios:\n",
    "            subset = robustness_curves.filter(pl.col(scenario_col) == scenario).to_pandas()\n",
    "            x_vals = subset[x_col].values\n",
    "            y_vals = subset[y_col].values\n",
    "            \n",
    "            # AUC (area under robustness curve)\n",
    "            auc = compute_auc(x_vals, y_vals)\n",
    "            \n",
    "            # Critical drop: fraction removed when LCC drops below 50%\n",
    "            below_50 = subset[subset[y_col] < 0.5]\n",
    "            critical_point = below_50[x_col].min() if len(below_50) > 0 else 1.0\n",
    "            \n",
    "            # Initial drop rate (slope at beginning)\n",
    "            if len(x_vals) > 1:\n",
    "                initial_slope = (y_vals[1] - y_vals[0]) / (x_vals[1] - x_vals[0] + 1e-10)\n",
    "            else:\n",
    "                initial_slope = 0\n",
    "            \n",
    "            summary_metrics.append({\n",
    "                \"scenario\": scenario,\n",
    "                \"auc_robustness\": auc,\n",
    "                \"critical_50pct_drop\": critical_point,\n",
    "                \"initial_slope\": initial_slope,\n",
    "                \"final_lcc\": y_vals[-1] if len(y_vals) > 0 else 0\n",
    "            })\n",
    "    else:\n",
    "        df = robustness_curves.to_pandas()\n",
    "        x_vals = df[x_col].values\n",
    "        y_vals = df[y_col].values\n",
    "        auc = compute_auc(x_vals, y_vals)\n",
    "        \n",
    "        summary_metrics.append({\n",
    "            \"scenario\": \"default\",\n",
    "            \"auc_robustness\": auc,\n",
    "            \"critical_50pct_drop\": df[df[y_col] < 0.5][x_col].min() if len(df[df[y_col] < 0.5]) > 0 else 1.0,\n",
    "            \"initial_slope\": 0,\n",
    "            \"final_lcc\": y_vals[-1] if len(y_vals) > 0 else 0\n",
    "        })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_metrics)\n",
    "if len(summary_df) > 0:\n",
    "    print(\"\\nROBUSTNESS SUMMARY METRICS:\")\n",
    "    display(summary_df)\n",
    "    \n",
    "    # Highlight key finding\n",
    "    if \"random\" in summary_df[\"scenario\"].str.lower().values and \"targeted\" in str(summary_df[\"scenario\"].str.lower().values):\n",
    "        random_auc = summary_df[summary_df[\"scenario\"].str.contains(\"random\", case=False)][\"auc_robustness\"].values[0]\n",
    "        targeted_auc = summary_df[summary_df[\"scenario\"].str.contains(\"targeted|degree|betweenness\", case=False)][\"auc_robustness\"].values\n",
    "        if len(targeted_auc) > 0:\n",
    "            auc_diff = random_auc - targeted_auc[0]\n",
    "            print(f\"\\nðŸ“Š HUB DEPENDENCE INDICATOR: Random AUC - Targeted AUC = {auc_diff:.3f}\")\n",
    "            print(f\"   (Larger positive values indicate greater vulnerability to targeted attacks)\")\n",
    "else:\n",
    "    print(\"Not available: could not compute summary metrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817043e6",
   "metadata": {},
   "source": [
    "<a id=\"critical\"></a>\n",
    "## 6. Critical Nodes Analysis\n",
    "\n",
    "Identify which nodes are most critical for network connectivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b542eb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CRITICAL NODES ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "# Check for critical nodes table from pipeline\n",
    "critical_nodes_file = RESULTS_DIR / \"tables\" / \"robustness_critical_nodes.csv\"\n",
    "\n",
    "if critical_nodes_file.exists():\n",
    "    critical_nodes = pd.read_csv(critical_nodes_file)\n",
    "    print(\"CRITICAL NODES (from pipeline):\")\n",
    "    display(critical_nodes.head(20))\n",
    "else:\n",
    "    print(\"Critical nodes table not found in pipeline outputs.\")\n",
    "    print(\"This analysis requires the robustness script to output removal order.\")\n",
    "    \n",
    "    # Fallback: use centrality as proxy for criticality\n",
    "    centrality_file = ANALYSIS_DIR / \"airport_centrality.parquet\"\n",
    "    if centrality_file.exists():\n",
    "        print(\"\\nUsing centrality as proxy for critical nodes:\")\n",
    "        centrality = safe_load_parquet(centrality_file)\n",
    "        if centrality is not None:\n",
    "            # Rank by betweenness if available, else degree\n",
    "            rank_col = next((c for c in centrality.columns if \"betweenness\" in c.lower()), None)\n",
    "            if not rank_col:\n",
    "                rank_col = next((c for c in centrality.columns if \"degree\" in c.lower() or \"strength\" in c.lower()), None)\n",
    "            \n",
    "            if rank_col:\n",
    "                id_col = next((c for c in [\"airport\", \"node\", \"id\"] if c in centrality.columns), centrality.columns[0])\n",
    "                top_critical = centrality.sort(rank_col, descending=True).head(20)\n",
    "                print(f\"\\nTop 20 by {rank_col}:\")\n",
    "                display(top_critical.to_pandas()[[id_col, rank_col]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9872c223",
   "metadata": {},
   "source": [
    "<a id=\"interpretation\"></a>\n",
    "## 7. Interpretation\n",
    "\n",
    "### Key Findings (Evidence-Grounded)\n",
    "\n",
    "*(Populated after running cells above)*\n",
    "\n",
    "### Mechanistic Explanation\n",
    "\n",
    "- **Random failure resilience**: Scale-free networks are robust to random node failures due to hub redundancy\n",
    "- **Targeted attack vulnerability**: Removing high-degree hubs fragments the network quickly\n",
    "- **Percolation threshold**: Critical point where giant component collapses\n",
    "\n",
    "### Alternative Explanations\n",
    "1. Weight definition affects which nodes appear most critical\n",
    "2. Dynamic recomputation of centrality during attacks may change results\n",
    "3. Airline-specific analysis might reveal different vulnerability patterns\n",
    "\n",
    "### Evidence Links\n",
    "- Figure: `results/figures/report/nb05_robustness_curves.png`\n",
    "- Table: `results/tables/report/nb05_robustness_summary_metrics.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9bc97c",
   "metadata": {},
   "source": [
    "<a id=\"write-outputs\"></a>\n",
    "## 8. Write Report Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ae9897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# WRITE REPORT OUTPUTS\n",
    "# ============================================================================\n",
    "\n",
    "# Write summary metrics\n",
    "if len(summary_df) > 0:\n",
    "    metrics_path = TABLES_REPORT_DIR / f\"{NOTEBOOK_ID}_robustness_summary_metrics.csv\"\n",
    "    summary_df.to_csv(metrics_path, index=False)\n",
    "    print(f\"âœ… Wrote: {metrics_path}\")\n",
    "\n",
    "print(f\"\\nðŸ“‹ All {NOTEBOOK_ID} outputs written.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ded51b",
   "metadata": {},
   "source": [
    "<a id=\"reproducibility\"></a>\n",
    "## 9. Reproducibility Notes\n",
    "\n",
    "### Input Files Consumed\n",
    "- `results/analysis/robustness_curves.parquet`\n",
    "- `results/analysis/robustness_summary.json`\n",
    "- `results/tables/robustness_critical_nodes.csv` (optional)\n",
    "\n",
    "### Assumptions Made\n",
    "1. Robustness computed on the largest connected component\n",
    "2. Targeted attacks use pre-computed centrality rankings (not recomputed dynamically)\n",
    "3. LCC fraction is relative to original network size\n",
    "\n",
    "### Metrics Definitions\n",
    "- **AUC**: Area under the robustness curve (higher = more robust)\n",
    "- **Critical 50% drop**: Fraction of nodes removed when LCC drops below 50%\n",
    "- **Initial slope**: Rate of LCC decline at start of attack sequence\n",
    "\n",
    "### Outputs Generated\n",
    "| Artifact | Path |\n",
    "|----------|------|\n",
    "| Robustness Curves | `results/figures/report/nb05_robustness_curves.png` |\n",
    "| Summary Metrics | `results/tables/report/nb05_robustness_summary_metrics.csv` |"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
