{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "919ff9d7",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. [Setup and Imports](#setup)\n",
    "2. [Discover Community Artifacts](#discover)\n",
    "3. [Load and Inspect Community Data](#load)\n",
    "4. [Community Size Distribution](#size-dist)\n",
    "5. [Dominant Airline Analysis](#airline)\n",
    "6. [Community Bridging Nodes](#bridging)\n",
    "7. [Interpretation](#interpretation)\n",
    "8. [Write Report Outputs](#write-outputs)\n",
    "9. [Reproducibility Notes](#reproducibility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7f56a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SETUP AND IMPORTS\n",
    "# ============================================================================\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Project paths\n",
    "REPO_ROOT = Path.cwd().parent.parent\n",
    "RESULTS_DIR = REPO_ROOT / \"results\"\n",
    "ANALYSIS_DIR = RESULTS_DIR / \"analysis\"\n",
    "TABLES_REPORT_DIR = RESULTS_DIR / \"tables\" / \"report\"\n",
    "FIGURES_REPORT_DIR = RESULTS_DIR / \"figures\" / \"report\"\n",
    "WARNINGS_LOG = TABLES_REPORT_DIR / \"_warnings.log\"\n",
    "\n",
    "# Notebook identity\n",
    "NOTEBOOK_ID = \"nb04\"\n",
    "NOTEBOOK_NAME = \"communities__structure_and_attributes\"\n",
    "\n",
    "# Plotting settings\n",
    "plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Ensure output directories exist\n",
    "TABLES_REPORT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "FIGURES_REPORT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Analysis dir exists: {ANALYSIS_DIR.exists()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094edc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# HELPER FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def append_warning(message: str, notebook_id: str = NOTEBOOK_ID):\n",
    "    \"\"\"Append a warning to the consolidated warnings log.\"\"\"\n",
    "    timestamp = datetime.now().isoformat()\n",
    "    with open(WARNINGS_LOG, \"a\") as f:\n",
    "        f.write(f\"[{timestamp}] [{notebook_id}] {message}\\n\")\n",
    "    print(f\"WARNING: {message}\")\n",
    "\n",
    "def safe_load_parquet(path: Path) -> pl.DataFrame | None:\n",
    "    \"\"\"Safely load a parquet file, returning None if it fails.\"\"\"\n",
    "    try:\n",
    "        return pl.read_parquet(path)\n",
    "    except Exception as e:\n",
    "        append_warning(f\"Failed to load {path.name}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815a5214",
   "metadata": {},
   "source": [
    "<a id=\"discover\"></a>\n",
    "## 2. Discover Community Artifacts\n",
    "\n",
    "Search for files containing community-related keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee8ca7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DISCOVER COMMUNITY ARTIFACTS\n",
    "# ============================================================================\n",
    "\n",
    "community_keywords = [\"community\", \"leiden\", \"partition\", \"membership\", \"cluster\", \"sbm\"]\n",
    "\n",
    "# Search in analysis directory\n",
    "analysis_files = list(ANALYSIS_DIR.glob(\"*.parquet\")) + list(ANALYSIS_DIR.glob(\"*.csv\"))\n",
    "community_candidates = [\n",
    "    f for f in analysis_files \n",
    "    if any(kw in f.name.lower() for kw in community_keywords)\n",
    "]\n",
    "\n",
    "print(f\"Found {len(community_candidates)} community-related artifacts:\")\n",
    "for cf in sorted(community_candidates):\n",
    "    print(f\"  - {cf.name}\")\n",
    "\n",
    "# Categorize by network type\n",
    "airport_community_files = [f for f in community_candidates if \"airport\" in f.name.lower()]\n",
    "flight_community_files = [f for f in community_candidates if \"flight\" in f.name.lower()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f377803",
   "metadata": {},
   "source": [
    "<a id=\"load\"></a>\n",
    "## 3. Load and Inspect Community Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4139df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# LOAD AND INSPECT COMMUNITY DATA\n",
    "# ============================================================================\n",
    "\n",
    "community_dfs = {}\n",
    "\n",
    "for cf in community_candidates:\n",
    "    if cf.suffix == \".parquet\":\n",
    "        df = safe_load_parquet(cf)\n",
    "        if df is not None:\n",
    "            community_dfs[cf.stem] = df\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"{cf.name}\")\n",
    "            print(f\"{'='*60}\")\n",
    "            print(f\"Shape: {df.shape}\")\n",
    "            print(f\"Columns: {df.columns}\")\n",
    "            display(df.head(5).to_pandas())\n",
    "\n",
    "if len(community_dfs) == 0:\n",
    "    append_warning(\"No community data could be loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6df1d9d",
   "metadata": {},
   "source": [
    "<a id=\"size-dist\"></a>\n",
    "## 4. Community Size Distribution\n",
    "\n",
    "Compute and visualize the distribution of community sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8101ac97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# COMMUNITY SIZE DISTRIBUTION\n",
    "# ============================================================================\n",
    "\n",
    "size_distributions = {}\n",
    "\n",
    "for name, df in community_dfs.items():\n",
    "    # Find community ID column\n",
    "    comm_col = next((c for c in [\"community\", \"community_id\", \"cluster\", \"partition\", \"leiden\"] \n",
    "                     if c in df.columns), None)\n",
    "    \n",
    "    if comm_col is None:\n",
    "        # Try to find any column that looks like community assignments\n",
    "        for c in df.columns:\n",
    "            if df[c].dtype in [pl.Int64, pl.Int32, pl.UInt32, pl.UInt64]:\n",
    "                unique_vals = df[c].n_unique()\n",
    "                if 2 < unique_vals < len(df) // 2:  # Reasonable number of communities\n",
    "                    comm_col = c\n",
    "                    break\n",
    "    \n",
    "    if comm_col:\n",
    "        # Compute community sizes\n",
    "        sizes = (\n",
    "            df.group_by(comm_col)\n",
    "            .agg(pl.count().alias(\"size\"))\n",
    "            .sort(\"size\", descending=True)\n",
    "            .with_row_index(\"rank\", offset=1)\n",
    "        )\n",
    "        \n",
    "        size_distributions[name] = {\n",
    "            \"data\": sizes.to_pandas(),\n",
    "            \"comm_col\": comm_col,\n",
    "            \"n_communities\": sizes.height,\n",
    "            \"largest\": sizes[\"size\"].max(),\n",
    "            \"smallest\": sizes[\"size\"].min()\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n{name}:\")\n",
    "        print(f\"  Community column: {comm_col}\")\n",
    "        print(f\"  Number of communities: {sizes.height}\")\n",
    "        print(f\"  Largest community: {sizes['size'].max()} nodes\")\n",
    "        print(f\"  Smallest community: {sizes['size'].min()} nodes\")\n",
    "    else:\n",
    "        append_warning(f\"Could not identify community column in {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da16b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PLOT COMMUNITY SIZE DISTRIBUTIONS\n",
    "# ============================================================================\n",
    "\n",
    "if len(size_distributions) > 0:\n",
    "    for name, info in size_distributions.items():\n",
    "        sizes_df = info[\"data\"]\n",
    "        \n",
    "        # Take top 30 for readability\n",
    "        top_30 = sizes_df.head(30)\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(12, 8))\n",
    "        \n",
    "        colors = sns.color_palette(\"coolwarm\", len(top_30))\n",
    "        bars = ax.bar(range(len(top_30)), top_30[\"size\"], color=colors)\n",
    "        \n",
    "        ax.set_xlabel(\"Community Rank\")\n",
    "        ax.set_ylabel(\"Community Size (nodes)\")\n",
    "        ax.set_title(f\"Community Size Distribution: {name}\\n(Top 30 of {info['n_communities']} communities)\")\n",
    "        \n",
    "        # Add community ID labels for top 10\n",
    "        ax.set_xticks(range(min(10, len(top_30))))\n",
    "        ax.set_xticklabels(top_30[info[\"comm_col\"]].head(10).astype(str), rotation=45)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        fig_path = FIGURES_REPORT_DIR / f\"{NOTEBOOK_ID}_community_sizes__{name}.png\"\n",
    "        plt.savefig(fig_path, dpi=150)\n",
    "        plt.show()\n",
    "        print(f\"âœ… Saved: {fig_path.name}\")\n",
    "else:\n",
    "    print(\"Not available: no community size data to plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c64e851",
   "metadata": {},
   "source": [
    "<a id=\"airline\"></a>\n",
    "## 5. Dominant Airline Analysis\n",
    "\n",
    "If airline attributes are available, identify the dominant airline per community."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5154fed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DOMINANT AIRLINE ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "airline_analysis_available = False\n",
    "\n",
    "for name, df in community_dfs.items():\n",
    "    # Check for airline column\n",
    "    airline_col = next((c for c in [\"carrier\", \"airline\", \"OP_UNIQUE_CARRIER\", \"op_carrier\"] \n",
    "                        if c in df.columns), None)\n",
    "    \n",
    "    comm_col = size_distributions.get(name, {}).get(\"comm_col\")\n",
    "    \n",
    "    if airline_col and comm_col:\n",
    "        airline_analysis_available = True\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"DOMINANT AIRLINE ANALYSIS: {name}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Compute dominant airline per community\n",
    "        dominant = (\n",
    "            df.group_by([comm_col, airline_col])\n",
    "            .agg(pl.count().alias(\"count\"))\n",
    "            .sort([comm_col, \"count\"], descending=[False, True])\n",
    "            .group_by(comm_col)\n",
    "            .first()\n",
    "        )\n",
    "        \n",
    "        # Add community size and compute share\n",
    "        comm_sizes = df.group_by(comm_col).agg(pl.count().alias(\"total_size\"))\n",
    "        dominant = dominant.join(comm_sizes, on=comm_col)\n",
    "        dominant = dominant.with_columns(\n",
    "            (pl.col(\"count\") / pl.col(\"total_size\")).alias(\"dominant_share\")\n",
    "        ).sort(\"total_size\", descending=True)\n",
    "        \n",
    "        print(f\"\\nTop 15 communities by size with dominant airline:\")\n",
    "        display(dominant.head(15).to_pandas())\n",
    "        \n",
    "        # Save for report\n",
    "        dominant_path = TABLES_REPORT_DIR / f\"{NOTEBOOK_ID}_community_dominant_airline__{name}.csv\"\n",
    "        dominant.to_pandas().to_csv(dominant_path, index=False)\n",
    "        print(f\"âœ… Saved: {dominant_path.name}\")\n",
    "    else:\n",
    "        if not airline_col:\n",
    "            print(f\"\\n{name}: No airline column found - dominant airline analysis not available\")\n",
    "\n",
    "if not airline_analysis_available:\n",
    "    print(\"\\nNot available: no community data has airline attributes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e990e3d",
   "metadata": {},
   "source": [
    "<a id=\"bridging\"></a>\n",
    "## 6. Community Bridging Nodes\n",
    "\n",
    "Identify nodes that may bridge between communities (for cross-reference with centrality)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897d59aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# COMMUNITY BRIDGING NODES (PLACEHOLDER)\n",
    "# ============================================================================\n",
    "\n",
    "# This analysis requires edge data to identify nodes connected to multiple communities\n",
    "# Will be synthesized in Notebook 09 with centrality data\n",
    "\n",
    "print(\"Community bridging analysis will be performed in Notebook 09 (Synthesis)\")\n",
    "print(\"This requires joining community assignments with edge data and centrality metrics.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96006a3e",
   "metadata": {},
   "source": [
    "<a id=\"interpretation\"></a>\n",
    "## 7. Interpretation\n",
    "\n",
    "### Key Findings (Evidence-Grounded)\n",
    "\n",
    "*(Populated after running cells above)*\n",
    "\n",
    "### Mechanistic Explanation\n",
    "\n",
    "- **Leiden algorithm**: Optimizes modularity with guaranteed community connectivity\n",
    "- **SBM (Stochastic Block Model)**: Probabilistic generative model for community structure\n",
    "- **Resolution parameter**: Controls granularity of detected communities\n",
    "\n",
    "### Alternative Explanations\n",
    "1. Carrier-dominated communities may reflect hub-and-spoke network design\n",
    "2. Geographic communities may emerge from regional travel patterns\n",
    "3. Many small communities may indicate over-resolution or network fragmentation\n",
    "\n",
    "### Evidence Links\n",
    "- Table: `results/tables/report/nb04_community_sizes.csv`\n",
    "- Figure: `results/figures/report/nb04_community_sizes_*.png`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7e01a7",
   "metadata": {},
   "source": [
    "<a id=\"write-outputs\"></a>\n",
    "## 8. Write Report Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aab140d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# WRITE REPORT OUTPUTS\n",
    "# ============================================================================\n",
    "\n",
    "# Combine all community size distributions\n",
    "if len(size_distributions) > 0:\n",
    "    combined_rows = []\n",
    "    for name, info in size_distributions.items():\n",
    "        sizes_df = info[\"data\"].copy()\n",
    "        sizes_df[\"source\"] = name\n",
    "        combined_rows.append(sizes_df)\n",
    "    \n",
    "    combined_df = pd.concat(combined_rows, ignore_index=True)\n",
    "    combined_path = TABLES_REPORT_DIR / f\"{NOTEBOOK_ID}_community_sizes.csv\"\n",
    "    combined_df.to_csv(combined_path, index=False)\n",
    "    print(f\"âœ… Wrote: {combined_path}\")\n",
    "\n",
    "print(f\"\\nðŸ“‹ All {NOTEBOOK_ID} outputs written.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2ac3c7",
   "metadata": {},
   "source": [
    "<a id=\"reproducibility\"></a>\n",
    "## 9. Reproducibility Notes\n",
    "\n",
    "### Input Files Consumed\n",
    "- `results/analysis/airport_leiden_membership.parquet`\n",
    "- `results/analysis/airport_sbm_membership.parquet`\n",
    "- `results/analysis/flight_leiden_membership.parquet`\n",
    "\n",
    "### Assumptions Made\n",
    "1. Community assignments are from a single run (deterministic with fixed seed)\n",
    "2. Node identifiers match between community and centrality tables\n",
    "3. Resolution parameters are as specified in config.yaml\n",
    "\n",
    "### Sorting/Ordering\n",
    "- Communities ranked by size descending\n",
    "- Stable tie-breaking by community ID ascending\n",
    "\n",
    "### Caveats\n",
    "- Resolution parameter choice affects number and size of communities\n",
    "- Different algorithms (Leiden vs SBM) may yield different partitions\n",
    "- Airline dominance is computed from node-level carrier attributes if available\n",
    "\n",
    "### Outputs Generated\n",
    "| Artifact | Path |\n",
    "|----------|------|\n",
    "| Community Sizes | `results/tables/report/nb04_community_sizes.csv` |\n",
    "| Size Distribution Figures | `results/figures/report/nb04_community_sizes__*.png` |\n",
    "| Dominant Airline | `results/tables/report/nb04_community_dominant_airline__*.csv` |"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
